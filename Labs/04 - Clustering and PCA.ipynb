{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Clustering, PCA, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we consider unsupervised learning in the form of clustering methods and principal component analysis (PCA), as well as more thorough performance evaluation of classifiers.\n",
    "\n",
    "All the datasets that you will need for this lab are located within the `datasets` directory (i.e. adjacent to this file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Clustering the landsat dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider clustering of the Landsat data. For information about the Landsat data you can read [this description](http://www.inf.ed.ac.uk/teaching/courses/dme/html/landsat.html). Since there are 6 classes in the data, it would be interesting to try clustering with k=6 centres..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 ==========\n",
    "1. With pandas, read the csv dataset located at './datasets/landsat.csv'\n",
    "1. Split the data into the features `X` (pandas dataframe), and the labels `y` (easier to make it a numpy array)\n",
    "1. Give it a once over\n",
    "    * Get a feel for the size\n",
    "    * Check it looks ok\n",
    "    * Understand what the features are\n",
    "    * Plot the class distribution\n",
    "\n",
    "*Hint: You might need to use the following class labels*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class Categories run from 1 through 7 inclusive (i.e. they are not 0 indexed!)\n",
    "class_categories = ['red soil', 'cotton crop', 'grey soil', 'damp grey soil', 'soil with vegetation stubble',\n",
    "                    'mixture class (all types present)', 'very damp grey soil']\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TIP: Do not assume that all labels are present!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TIP - don't get stuck on this, move on after 10 mins or so (it's not critical)*\n",
    "\n",
    "Plot a few datapoints. You'll need to understand and reshape the datapoints to do this. *Hint: try reading the [detailed description](http://www.inf.ed.ac.uk/teaching/courses/dme/html/satdoc.txt), it'll take you 2 minutes...`plt.imshow()` or `sns.heatmap()` (with an `ax.invert_yaxis()`) may also be handy ;)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ========== Question 1.3 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Read [this demonstration of k-means clustering assumptions](http://scikit-learn.org/0.19/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py) from the sklearn documentation. Get a feel for how to create and fit a k-means object and what the different arguments do.\n",
    "\n",
    "Initialise a [k-means clustering](http://scikit-learn.org/0.19/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) object with 6 clusters, and one other parameter that ensures you can reproduce your results (other arguments kept as default). Call the object `kmeans`. Use the `fit()` method to fit to the training data (`X` - the features of `landsat` created above)\n",
    "\n",
    "**Be careful to fit `X` - only the features - not the class labels! And Remember to set the `random_state=1000` so that you can reproduce results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn documentation gives a great introduction to k-means [here](http://scikit-learn.org/0.19/modules/clustering.html#k-means). It describes what the algorithm is trying to minimise - the squared difference between datapoints and their closest cluster centre - a.k.a. the **inertia**. Lower inertia implies a better fit.\n",
    "\n",
    "So, how well did that work? Are the classes well separated and form 6 nice clusters? Since we have the true class labels in this case, we can use the [adjusted rand index](http://scikit-learn.org/0.19/modules/clustering.html#clustering-performance-evaluation) metric. Understand what it is and roughly how it is calculated (try the [mathematical formulation on sklearn](http://scikit-learn.org/0.19/modules/clustering.html#mathematical-formulation) or [on wikipedia](https://en.wikipedia.org/wiki/Rand_index)).\n",
    "\n",
    "Print the `inertia` and the `adjusted_rand_score` of the kmeans object. The inertia is contained within the `kmeans` object you just fitted as a property. You need to use `y` and the cluster labels (another property of the `kmeans` object you just made), and are welcome to use the sklearn metrics function [adjusted_rand_score](http://scikit-learn.org/0.19/modules/generated/sklearn.metrics.adjusted_rand_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the counts of the labels within each cluster. If the clustering has worked well, and the labels are inticative of genuine difference in the data, we should expect each cluster to have one dominant label.\n",
    "\n",
    "Use `sns.countplot` on `kmeans.labels_` with a hue of `y` to get a plot that counts the number of instances within each cluster, and breaks them down by the class labels.\n",
    "\n",
    "Below the plot, comment on:\n",
    "1. How successful the clustering has been at separating data with different labels\n",
    "1. Focussing on clusters, which are the best?\n",
    "1. Focussing on labels, which are well identified by the clustering?\n",
    "1. Which labels are the hardest to determine by the data point cluster assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The landsat data is 36 dimensional, so we cannot visualise it, with respect to class, on a nice two dimensional plot. Additionally, as dimensionality increases, euclidean distance [becomes less meaningful](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)...\n",
    "\n",
    "Perhaps if we found a lower dimensional subspace the data lies upon, we could more easily distinguish the datapoints..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the [PCA example](http://scikit-learn.org/0.19/auto_examples/decomposition/plot_pca_iris.html) in the sklearn documentation. For more information about PCA and decomposition in general check out the sklearn [user guide on decomposition](http://scikit-learn.org/0.19/modules/decomposition.html#pca).\n",
    "\n",
    "We are going to project the data down to 2 dimensions and visualise it using PCA. \n",
    "\n",
    "1. Create `pca`, an instance of an [sklearn PCA object](http://scikit-learn.org/0.19/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA), setting n_components to 2. \n",
    "1. Create `X_2d` by using the pca method `fit_transform()` and supplying the features `X` to fit and transform to 2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the data! Use a scatterplot and colour the datapoints by their class. You'll find [this example](http://scikit-learn.org/0.19/auto_examples/decomposition/plot_pca_vs_lda.html) very helpful to adapt. \n",
    "\n",
    "Below the plot, comment on whether the data looks more or less seperable now. Do the data look like they will be confused by a k-means clustering in the same way now?\n",
    "\n",
    "**Extension**: Unless the data is somehow magically perfectly seperable, you may want to try and describe the space a little better than a scatterplot (bacause points are plotted on top of one another). Try and make a plot that clarifies the location of the classes. *Hint: We're actually interested in where the **density** is.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*\n",
    "\n",
    "Without performing a clustering, it's hard to say whether the data is more or less seperable. However, we can see 6 definite clusters with some overlap in the top plot, but it's not all that clear. The kernel density plot allows us to see better where the density lies. These clusters are not spherical and there is overlap, so k-means will always fail. See in particular the 'red soil' data points vs. the 'soil with vegetation stubble points. As expected, we see the 'cotton crop' data points are very distinct from the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out whether it's easier to model the transformed data. Fit k-means to the transformed data and report the inertia and the adjusted rand index. Below, comment on whether it is better or worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blobs in the 2 dimensional plot do look quite gaussian...try another classifier on the 2d data and see if it can perform better. What about using 3 principal component? \n",
    "\n",
    "Maybe there are subclasses within each class? Maybe increasing the number of clusters will increase your `adjusted_rand_score`.\n",
    "\n",
    "Use the adjusted rand score for fair comparison. Why do you think it works better or worse? Discuss with your colleagues and lab tutors why you think you got better/worse/the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STARTHIDE#\n",
    "# Here are just some examples you could try: Anything goes for this question\n",
    "# (1) Try different number of cluster for 2D PCA\n",
    "print('=========== Number of Clusters ===========\\n')\n",
    "for ii in range(1,20,2):\n",
    "    km = KMeans(n_clusters=ii, random_state=1000)  \n",
    "    km.fit(X_2d)\n",
    "    y_pred = km.labels_\n",
    "    print('k-means with {} clusters on 2D PCA:\\nARI: {}, Inertia: {}\\n\\n'.\n",
    "          format(ii, adjusted_rand_score(y, y_pred), km.inertia_))\n",
    "\n",
    "# (2) Try 6 clusters for different PCA Components\n",
    "print('\\n======== Number of PCA Components ========\\n')\n",
    "for ii in range(2, 6):\n",
    "    X_pca = PCA(n_components = ii).fit_transform(X)\n",
    "    km = KMeans(n_clusters=6, random_state=1000)  \n",
    "    km.fit(X_pca)\n",
    "    y_pred = km.labels_\n",
    "    print('k-means with 6 clusters on {}D PCA:\\nARI: {}, Inertia: {}\\n\\n'.\n",
    "          format(ii, adjusted_rand_score(y, y_pred), km.inertia_))\n",
    "    \n",
    "# (3) Try Gaussian NB for different PCA Components\n",
    "print('\\n======== Gaussian Naive Bayes PCA ========\\n')\n",
    "for ii in range(2, 6):\n",
    "    X_pca = PCA(n_components = ii).fit_transform(X)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_pca, y)\n",
    "    y_pred = gnb.predict(X_pca)\n",
    "    print('Gaussian Naive Bayes on {}D PCA:\\nARI: {}, Mean Accuracy: {}\\n\\n'.\n",
    "          format(ii, adjusted_rand_score(y, y_pred), gnb.score(X_pca, y)))\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "y_pred = gnb.predict(X)\n",
    "print('Gaussian Naive Bayes on Full Data\\nARI: {}, Mean Accuracy: {}\\n\\n'.\n",
    "      format(adjusted_rand_score(y, y_pred), gnb.score(X, y)))\n",
    "\n",
    "\n",
    "# (4) Try Random Forests\n",
    "for ii in range(2, 6):\n",
    "    X_pca = PCA(n_components = ii).fit_transform(X)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=1337, oob_score=True)\n",
    "    rf.fit(X_pca, y)\n",
    "    y_pred = np.argmax(rf.oob_decision_function_, axis=1)\n",
    "    print('Random Forests on {}D PCA:\\nARI: {}, Mean Accuracy: {}\\n\\n'.\n",
    "          format(ii, adjusted_rand_score(y, y_pred), rf.oob_score_))\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1000, oob_score=True)\n",
    "rf.fit(X, y)\n",
    "y_pred = np.argmax(rf.oob_decision_function_, axis=1)\n",
    "print('Random Forest on Full Data\\nARI: {}, Mean Accuracy: {}\\n\\n'.\n",
    "      format(adjusted_rand_score(y, y_pred), rf.oob_score_))\n",
    "\n",
    "# (5) Finally Draw the distribution of each label in the 3D PCA space\n",
    "X_3d = PCA(n_components=3).fit(X).transform(X)\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=30, azim=300)\n",
    "for label, cat in zip(sub_labels, sub_cats):\n",
    "    idx = y == label\n",
    "    ax.scatter(X_3d[idx, 0], X_3d[idx, 1], X_3d[idx, 2], label = cat)\n",
    "plt.legend(loc='center left', bbox_to_anchor=[1.1, .5])\n",
    "plt.show()\n",
    "#ENDHIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab prepared by Lawrence Murray and Chris Williams, November 2008; revised Athina Spiliopoulou Nov 2009; revised Sean Moran Nov 2011; revised Boris Mitrovic Oct 2013; revised and converted python by James Owers and Agamemnon Krasoulis Oct 2016; revised and updated to python 3 by Michael Camilleri"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}